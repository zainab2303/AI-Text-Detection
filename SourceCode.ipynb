{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re \n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # lowercase\n",
    "    text = re.sub(r'[^a-zA-Z0-9,.?! ]', '', text)  # Removing punctuation except ,.?! and space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  #Removing extra whitespaces\n",
    "    return text\n",
    "\n",
    "# tokenization\n",
    "def add_tokens(text):\n",
    "    return \"<START> \" + text + \" <END>\"\n",
    "\n",
    "# Loading data\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = file.readlines()\n",
    "    return data\n",
    "\n",
    "# Cleaning and tokenizing\n",
    "def clean_tokenize_data(data):\n",
    "    cleaned_data = [clean_text(doc) for doc in data]\n",
    "    tokenized_data = [add_tokens(doc) for doc in cleaned_data]\n",
    "    return tokenized_data\n",
    "\n",
    "# Partitioning\n",
    "def partition_data(data, train_percent):\n",
    "    total_docs = len(data)\n",
    "    train_size = int(total_docs * train_percent)\n",
    "    train_data = data[:train_size]\n",
    "    test_data = data[train_size:]\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "human_file = 'hum.txt'\n",
    "gpt_file = 'gpt.txt'\n",
    "\n",
    "\n",
    "human_data = load_data(human_file)\n",
    "gpt_data = load_data(gpt_file)\n",
    "\n",
    "cleaned_human_data = clean_tokenize_data(human_data)\n",
    "cleaned_gpt_data = clean_tokenize_data(gpt_data)\n",
    "\n",
    "all_data = cleaned_human_data + cleaned_gpt_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition into training and test sets\n",
    "train_data_human, test_data_human = partition_data(cleaned_human_data, train_percent=0.9)\n",
    "\n",
    "train_data_gpt, test_data_gpt = partition_data(cleaned_gpt_data, train_percent=0.9)\n",
    "\n",
    "test_data = test_data_human+ test_data_gpt\n",
    "train_data = train_data_human + train_data_gpt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building Vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(documents):\n",
    "    vocabulary = set()\n",
    "    for doc in documents:\n",
    "        vocabulary.update(doc.split())\n",
    "    return vocabulary\n",
    "\n",
    "v = build_vocabulary(all_data)\n",
    "v_list = list(v)\n",
    "\n",
    "size_of_vocab=len(v_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Counting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_unigram(data):\n",
    "    word_frequency = {}\n",
    "    for document in data:\n",
    "     \n",
    "        for word in document.split():\n",
    "            # Count occurrences of each word\n",
    "            if word in word_frequency:\n",
    "                word_frequency[word] += 1\n",
    "            else:\n",
    "                word_frequency[word] = 1\n",
    "    return word_frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_count_unigram_train = count_unigram(train_data_human)\n",
    "gpt_count_unigram_train = count_unigram(train_data_gpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_bigram(data):\n",
    "    word_pairs_frequency = {}\n",
    "    for document in data:\n",
    "        words = document.split()\n",
    "        \n",
    "        for i in range(len(words) - 1):\n",
    "            word_pair = (words[i], words[i+1])\n",
    "            \n",
    "            if word_pair in word_pairs_frequency:\n",
    "                word_pairs_frequency[word_pair] += 1\n",
    "            else:\n",
    "                word_pairs_frequency[word_pair]=1\n",
    "    return word_pairs_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_count_bigram_train = count_bigram(train_data_human)\n",
    "gpt_count_bigram_train = count_bigram(train_data_gpt)\n",
    "\n",
    "bigrams = count_bigram(all_data)\n",
    "bigram_human = count_bigram(cleaned_human_data)\n",
    "bigram_gpt = count_bigram(cleaned_gpt_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trigram(data):\n",
    "    word_trigrams_frequency = {}\n",
    "    for document in data:\n",
    "        words = document.split()\n",
    "        \n",
    "        for i in range(len(words) - 2):\n",
    "            word_trigram = (words[i], words[i+1], words[i+2])\n",
    "            if word_trigram in word_trigrams_frequency:\n",
    "            # Count occurrences of each trigram\n",
    "                word_trigrams_frequency[word_trigram] += 1\n",
    "            else:\n",
    "                word_trigrams_frequency[word_trigram] = 1\n",
    "    return word_trigrams_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_count_trigram_train = count_trigram(train_data_human)\n",
    "gpt_count_trigram_train= count_trigram(train_data_gpt)\n",
    "trigram_human = count_trigram(cleaned_human_data)\n",
    "trigram_gpt = count_trigram(cleaned_gpt_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OOV Rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trigrams = count_trigram(train_data)\n",
    "train_bigrams = count_bigram(train_data)\n",
    "test_trigrams = count_trigram(test_data)\n",
    "test_bigrams = count_bigram(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV rate for bigrams: 9.61161054240112\n",
      "OOV rate for trigrams: 35.245720112894254\n"
     ]
    }
   ],
   "source": [
    "def calculate_oov_rate(test_bigrams, test_trigrams, train_bigrams, train_trigrams):\n",
    "    #Total Count\n",
    "    total_test_bigrams_with_repeats = sum(test_bigrams.values())\n",
    "    total_test_trigrams_with_repeats = sum(test_trigrams.values())\n",
    "    \n",
    "    # Converting to set\n",
    "    train_bigrams_set = set(train_bigrams.keys())\n",
    "    train_trigrams_set = set(train_trigrams.keys())\n",
    "    \n",
    "    oov_bigrams = 0\n",
    "    oov_trigrams = 0\n",
    "    \n",
    "    # Count OOV bigrams\n",
    "    for bigram in test_bigrams:\n",
    "        if bigram not in train_bigrams_set:\n",
    "            oov_bigrams += test_bigrams[bigram]  # Count with repeats\n",
    "    \n",
    "    # Count OOV trigrams\n",
    "    for trigram in test_trigrams:\n",
    "        if trigram not in train_trigrams_set:\n",
    "            oov_trigrams += test_trigrams[trigram]  # Count with repeats\n",
    "    \n",
    "    # Calculating OOV rates\n",
    "    oov_rate_bigrams = (oov_bigrams / total_test_bigrams_with_repeats) * 100\n",
    "    oov_rate_trigrams = (oov_trigrams / total_test_trigrams_with_repeats) * 100\n",
    "    \n",
    "    return oov_rate_bigrams, oov_rate_trigrams\n",
    "\n",
    "\n",
    "oov_rate_bigrams, oov_rate_trigrams = calculate_oov_rate(test_bigrams, test_trigrams, train_bigrams, train_trigrams)\n",
    "print(\"OOV rate for bigrams:\", oov_rate_bigrams)\n",
    "print(\"OOV rate for trigrams:\", oov_rate_trigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bigram Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1 with laplacian smoothing\n",
    "def laplacian_smoothing_bigram(unigram_count, bigram_count):\n",
    "    conditional_probablity = (bigram_count + 1)/(unigram_count + size_of_vocab)\n",
    "    return conditional_probablity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2 of bigram\n",
    "#Taking log for faster computation\n",
    "def prob_of_document_bigram(document ,unigram_corpus, bigram_corpus):\n",
    "    words = document.split()\n",
    "    log_prob = 0\n",
    "    for i in range(len(words) - 1):\n",
    "        unigram_count = unigram_corpus.get((words[i+1]),0)\n",
    "        bigram_count = bigram_corpus.get((words[i],words[i+1]),0)\n",
    "        conditional_probablity_word = laplacian_smoothing_bigram(unigram_count, bigram_count)\n",
    "        log_prob += math.log(conditional_probablity_word)\n",
    "        \n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_gpt = len(cleaned_gpt_data)\n",
    "\n",
    "total_human = len(cleaned_human_data)\n",
    "\n",
    "total = len(all_data)\n",
    "\n",
    "prob_of_human = total_human/total\n",
    "prob_of_gpt = total_gpt/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_classifier(document):\n",
    "    \n",
    "    max_class = None\n",
    "    log_prob_gpt = math.log(prob_of_gpt)+prob_of_document_bigram(document, gpt_count_unigram_train, gpt_count_bigram_train)\n",
    "    log_prob_human = math.log(prob_of_human) + prob_of_document_bigram(document, human_count_unigram_train, human_count_bigram_train)\n",
    "    \n",
    "    if log_prob_gpt >=log_prob_human:\n",
    "            max_class = \"GPT\"\n",
    "    else:\n",
    "          max_class = \"Human\"\n",
    "    return max_class\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Bigram: 0.9592481703260146\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = 0\n",
    "total_predictions = len(test_data)\n",
    "for document in test_data:\n",
    "    predicted_class = bigram_classifier(document)\n",
    "    actual_class = \"Human\" if document in cleaned_human_data else \"GPT\"\n",
    "    if predicted_class == actual_class:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(\"Accuracy of Bigram:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trigram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1 of trigram\n",
    "def laplacian_smoothing_trigram(bigram_count, trigram_count):\n",
    "    conditional_probability = (trigram_count+1)/ (bigram_count+size_of_vocab)\n",
    "    return conditional_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2 of trigram\n",
    "#Taking Log to avoid zero hence imporve the quality of the classifier\n",
    "def prob_of_document_trigram(document, bigram_corpus, trigram_corpus):\n",
    "    words = document.split()\n",
    "    log_prob = 0\n",
    "    for i in range(len(words) - 2):\n",
    "        bigram_count = bigram_corpus.get((words[i+1], words[i+2]), 0)\n",
    "        trigram_count = trigram_corpus.get((words[i], words[i+1], words[i+2]), 0)\n",
    "        conditional_probability_word = laplacian_smoothing_trigram(bigram_count, trigram_count)\n",
    "        log_prob += math.log(conditional_probability_word)\n",
    "    return log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_classifier(document):\n",
    "    max_class = None\n",
    "    log_prob_gpt = math.log(prob_of_gpt) + prob_of_document_trigram(document, gpt_count_bigram_train, gpt_count_trigram_train)\n",
    "    log_prob_human = math.log(prob_of_human) + prob_of_document_trigram(document, human_count_bigram_train, human_count_trigram_train)\n",
    "    \n",
    "    if log_prob_gpt >= log_prob_human:\n",
    "        max_class = \"GPT\"\n",
    "    else:\n",
    "        max_class = \"Human\"\n",
    "    \n",
    "    return max_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Trigram: 0.9421157684630739\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = 0\n",
    "total_predictions = len(test_data)\n",
    "for document in test_data:\n",
    "    predicted_class = trigram_classifier(document)\n",
    "    actual_class = \"Human\" if document in cleaned_human_data else \"GPT\"\n",
    "    if predicted_class == actual_class:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(\"Accuracy of Trigram:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def probability_distribution_bigram(word, T, bigrams):\n",
    "    denominator = 0\n",
    "    unnormalized_probs = {}\n",
    "\n",
    "    for bigram, count in bigrams.items():\n",
    "        if word in bigram:\n",
    "            \n",
    "            denominator += np.exp(count / T)  \n",
    "            \n",
    "    for bigram, count in bigrams.items():\n",
    "        #Handle overflow error\n",
    "        if word in bigram:\n",
    "            try:\n",
    "                numerator = np.exp(count / T)\n",
    "            except OverflowError:\n",
    "                numerator = 0.0\n",
    "            \n",
    "            \n",
    "            \n",
    "            start_index = bigram.index(word)\n",
    "            \n",
    "            other_word = bigram[1 - start_index]\n",
    "            #Handle Ovelflow error\n",
    "            try:\n",
    "                unnormalized_probs[other_word] = numerator / denominator\n",
    "            except OverflowError:\n",
    "                unnormalized_probs[other_word] = 0.0\n",
    "            \n",
    "    total_prob = sum(unnormalized_probs.values())\n",
    "    \n",
    "    if total_prob == 0.0:\n",
    "        # If total probability is 0, assign equal probabilities to all words\n",
    "        num_words = len(unnormalized_probs)\n",
    "        probabilities = {word: 1 / num_words for word in unnormalized_probs.keys()}\n",
    "    else:\n",
    "        # Otherwise, normalize the probabilities\n",
    "        probabilities = {bigram: prob / total_prob for bigram, prob in unnormalized_probs.items()}\n",
    "        \n",
    "    return probabilities\n",
    "\n",
    "\n",
    "#prob = probability_distribution_bigram('<START>', T=50, bigrams)\n",
    "#print(prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probability distribution trigram\n",
    "def probability_distribution_trigram(word, T, trigrams):\n",
    "    denominator = 0\n",
    "    unnormalized_probs = {}\n",
    "\n",
    "    for trigram, count in trigrams.items():\n",
    "        if word in trigram:\n",
    "            \n",
    "            denominator += np.exp(count / T)  \n",
    "            \n",
    "    for trigram, count in trigrams.items():\n",
    "        if word in trigram:\n",
    "            \n",
    "            numerator = np.exp(count / T)\n",
    "           \n",
    "            \n",
    "            start_index = trigram.index(word)\n",
    "        \n",
    "            other_word = trigram[1 - start_index]\n",
    "            \n",
    "            unnormalized_probs[other_word] = numerator / denominator\n",
    "            \n",
    "    total_prob = sum(unnormalized_probs.values())\n",
    "    \n",
    "    \n",
    "    probabilities = {bigram: prob / total_prob for bigram, prob in unnormalized_probs.items()}\n",
    "        \n",
    "    return probabilities\n",
    "\n",
    "#T = 50\n",
    "#prob = probability_distribution_trigram('<START>', T, trigram_human)\n",
    "#print(prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sentence for bigram_human: <START> the the the the the the the the the the the the the the the the the the the the <END>\n",
      "Generated sentence for bigram_human: <START> the the the the the the the the the the the the the the the the the the the the <END>\n",
      "Generated sentence for bigram_human: <START> the the the the the the the the the the the the the the the the the the the the <END>\n",
      "Generated sentence for bigram_human: <START> the the the the the the the the the the the the the the the the the the the the <END>\n",
      "Generated sentence for bigram_human: <START> the the the the the the the the the the the the the the the the the the the the <END>\n",
      "Generated sentence for bigram_gpt: <START> the the the the the the the the the the the the the the the the the the the the <END>\n",
      "Generated sentence for bigram_gpt: <START> the the the the the the the the the the the the the the the the the the the the <END>\n",
      "Generated sentence for bigram_gpt: <START> the the the the the the the the the the the the the the the the the the the the <END>\n",
      "Generated sentence for bigram_gpt: <START> the the the the the the the the the the the the the the the the the the the the <END>\n",
      "Generated sentence for bigram_gpt: <START> the the the the the the the the the the the the the the the the the the the the <END>\n",
      "Generated sentence for trigram_human: <START> stage applied e85 pal vanilla say acetaminophen birds gold george martin professor computability catholics matter essential cousin cherries braces leeroy <END>\n",
      "Generated sentence for trigram_human: <START> fleas fertility sequestration httpswww nice 2x4 quit have pile amir true hithe unsecured drying albany vegetable adverse infinity almost demographers <END>\n",
      "Generated sentence for trigram_human: <START> hickory eli4 angle sigh rapidly placeholder eating professor nanotechnology money rogue he3 acidity fads elevator mammals 27 samurai manufacturers bain <END>\n",
      "Generated sentence for trigram_human: <START> enigma brand earwax charging super broken intelligent incentive nanotechnology andorra eh eli16 fargo wind komodo jerker tacoma google simplifying paypal <END>\n",
      "Generated sentence for trigram_human: <START> scrip microagression sex distilled patrick chaos empire raffaello hypnotism cooking paranoia simulationbased leaks tumors welding iran approximately economically break cats <END>\n",
      "Generated sentence for trigram_gpt: <START> spiders st westminster milk goosebumps many android yes daniel precious hotels david deliverables yes betting libya quotational threephase before memory <END>\n",
      "Generated sentence for trigram_gpt: <START> japan sweat urls linux zyrtec primary explainable neoliberalism to drip ayn heroin headache yes yes kazakhstan yes shane uranium api <END>\n",
      "Generated sentence for trigram_gpt: <START> afghanistan comedy aol yes ocd absurdism wealthy e panadol flipflopping adolf enduser uber owls epilepsy aspartame neurons bohemian mending rogaine <END>\n",
      "Generated sentence for trigram_gpt: <START> polynomial bernie headaches reli5 grahams fire pulled popes html5 shakespeare yes yes inactive south music regulators cardio animal batteries defragmentation <END>\n",
      "Generated sentence for trigram_gpt: <START> san nonprofit transformational wrecking skins shaders carnivores ip holograms uncle quark sec printers neoliberalism betty insider some lump yes blue <END>\n"
     ]
    }
   ],
   "source": [
    "#Defining the constant vaiable\n",
    "T= 50\n",
    "\n",
    "def generate_sentence( probability_distribution_function,ngram_dict,max_length=20):\n",
    "    sentence = ['<START>']  \n",
    "    current_word = '<START>'\n",
    "    \n",
    "    # Generating words until reaching the maximum length or encountering the '<END>' token\n",
    "    for _ in range(max_length):\n",
    "        # Check if the current word is '<END>', if so, stop generating\n",
    "        if current_word == '<END>':\n",
    "            break\n",
    "        \n",
    "        # Getting the prob distribution for the current word\n",
    "        dist = probability_distribution_function(current_word, T,ngram_dict)\n",
    "        \n",
    "        # choosing the next word based by randomly sampling from np.random\n",
    "        next_word = np.random.choice(list(dist.keys()), p=list(dist.values()))\n",
    "        \n",
    "        \n",
    "        sentence.append(next_word)\n",
    "        \n",
    "        #To avoid <START> being generated in the middle of the sentence\n",
    "        if current_word == '<START>':\n",
    "            continue\n",
    "        else:\n",
    "            current_word = next_word\n",
    "    \n",
    "    # Ensure the sentence ends with the '<END>' token\n",
    "    if sentence[-1] != '<END>':\n",
    "        sentence.append('<END>')\n",
    "    \n",
    "    # Return the generated sentence as a string\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Generating 5 sentences for bigram_human\n",
    "for _ in range(5):\n",
    "    generated_sentence = generate_sentence(probability_distribution_bigram, bigram_human, max_length=20)\n",
    "    print(\"Generated sentence for bigram_human:\", generated_sentence)\n",
    "\n",
    "# Generating 5 sentences for bigram_gpt\n",
    "for _ in range(5):\n",
    "    generated_sentence = generate_sentence(probability_distribution_bigram, bigram_gpt, max_length=20)\n",
    "    print(\"Generated sentence for bigram_gpt:\", generated_sentence)\n",
    "\n",
    "# Generating 5 sentences for trigram_human\n",
    "for _ in range(5):\n",
    "    generated_sentence = generate_sentence(probability_distribution_trigram, trigram_human, max_length=20)\n",
    "    print(\"Generated sentence for trigram_human:\", generated_sentence)\n",
    "\n",
    "# Generating 5 sentences for trigram_gpt\n",
    "for _ in range(5):\n",
    "    generated_sentence = generate_sentence(probability_distribution_trigram, trigram_gpt, max_length=20)\n",
    "    print(\"Generated sentence for trigram_gpt:\", generated_sentence)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
